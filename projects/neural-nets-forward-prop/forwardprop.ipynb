{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project you will utilize the features of the Jupyter IDE and numpy to carry out vectorized forward propagation in neural networks. Forward propagation is an iterative operation where the output of one iteration (also called a layer) becomes the input to the next. Each iteration accepts two matricies as parameters, the weights $W$ and biases $b$, and one *activation* function $f(z)$. The output $Y$ of one iteration is related to the input $X$ through,\n",
    "\n",
    "$$Y = f(W X + b)$$\n",
    "\n",
    "where the product between $W$ and $X$ is matrix multiplication. We will do forward propagation for a network containing two layers. So the final output is,\n",
    "\n",
    "$$ \n",
    "\\begin{eqnarray}\n",
    "Y_0 &:=& X \\\\\n",
    "Y_1 &:=& f_1(W_1 X + b_1) \\\\\n",
    "Y_2 &:=& f_2(W_2 Y_1 + b_2) \\\\\n",
    "Y_2 &=& f_2 \\left[ W_2 f_1 \\left( W_1 X + b_1 \\right) + b_2 \\right] \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where subscripts denote which iteration the parameters and activation function refers to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "To start, open the `data.csv` file contained in this folder. Using the headers and numbers, get a sense for the sizes of the matrix and what the relationship is between the two inputs A and B and the output column for each row. Then, load the csv in as a numpy array, and separate it into one 2D array representing the inputs A,B and another 2D array representing the expected outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%logstart\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data.csv here\n",
    "X = None\n",
    "Y_true = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the data loaded and split into inputs and outputs. Use the following weights and biases to compute the input to the first activation function $Z_1 = W_1X + b_1$. Note: the multiplication happening between $W_1$ and $X$ is a dot product, not elementwise multiplication. You'll have to investigate the shape of $X$, $W_1$, and $b_1$ to ensure this can be carried out safely. If it can't, it's likely that one or more matrices are transposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights and biases, calculate Z_1\n",
    "W_1 = np.array([[2,2],[2,2]])\n",
    "b_1 = np.array([[-1],[-3]])\n",
    "Z_1 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a vectorized activiation function called ReLU, which stands for rectified linear unit. Mathetically, the function should return 0 if the argument is negative, and the argument's value if the argument is positive. Just remember, you're looking for a vectorized form of this function. Take advantage of the built in functions and/or operators numpy provides for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the ReLU activiation function\n",
    "def relu(Z):\n",
    "    return None\n",
    "X_2 = relu(Z_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the first iteration done, now we can carry out the same steps for the second iteration. This time, implement the sigmoid activiation function, which is defined as\n",
    "\n",
    "$f(x) = \\frac{1}{1 + exp(-x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights and biases for layer 2, calculate Z_2\n",
    "W_2 = np.array([[2,-6]])\n",
    "b_2 = np.array([[-1]])\n",
    "Z_2 = None\n",
    "\n",
    "# implement the sigmoid activation function\n",
    "def sigmoid(Z):\n",
    "    return None\n",
    "X_3 = sigmoid(Z_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the output of our network is usually interpretted as a probability. To get a prediction from this probability, we round to the nearest whole integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the probabilstic outcome of the network X_3 into predictions\n",
    "Y_pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
