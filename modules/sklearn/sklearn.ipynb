{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn\n",
    "\n",
    "This module focuses on the machine learning work hourse of Python: scikit-learn (also called sklearn). Scikit-learn is not part of the SciPy ecosystem but is built on top of several of it's major players including numpy, scipy, and matplotlib. If you're in need of any of the common machine learning methodologies or utilities you will probably find them here. The things you will not find are cutting edge and/or niche models and tools. Most notably, most requirements for reinforcement learning or deep learning are not going to be found in sklearn.\n",
    "\n",
    "To learn about the major components of scikit-learn, we will walk through a small project from beginning to end. Along the way there will be exercises highlighting common components of the workflow. Once complete, you will practice the entire workflow by completing another project independently or in small groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Workflow\n",
    "\n",
    "The basic workflow of machine learning often involves these steps:\n",
    "1. Exploration: Getting acquinted with the data attributes, values, and problem(s).\n",
    "2. Pre Processing: Transforming the original dataset to better suit your project's needs.\n",
    "3. Data Subsetting: Splitting the dataset into training, test, and possibly other subsets.\n",
    "4. Model Selection: Constraining the set of models to train on.\n",
    "5. Model Training: Learning from the dataset to tune model parameters.\n",
    "6. Model Analysis: Evaluating and picking the best model and parameters.\n",
    "\n",
    "The workflow is often not sequential, and depending on the data and context different steps will take varying amounts of effort to complete. However, all of the steps are usually present in some form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Iris dataset\n",
    "\n",
    "For our tutorial project, we will use the [iris data set](https://scikit-learn.org/stable/datasets/index.html#iris-dataset). The cell below loads the dataset for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['species'] = iris.target\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "\n",
    "Your first step will be to explore the dataset using the previous modules tools. Since we've looked at this dataset in previous modules, you can jump ahead once you're comfortable. \n",
    "\n",
    "**EXERCISE:**\n",
    "Make sure you can answer the following questions:\n",
    "\n",
    "1. How many records are there?\n",
    "2. How many attributes does each record have?\n",
    "3. What is the meaning and datatype of each attribute?\n",
    "3. What is the target attribute for prediction?\n",
    "4. How many unique values are present for each attribute?\n",
    "5. For attributes with low unique counts, what are the possible values and how frequent is each?\n",
    "6. For quantiative attributes, plot their distribution and common statistics (mean, quartiles, etc.).\n",
    "7. Do any attributes correlate highly with the target?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing\n",
    "\n",
    "This dataset is already fairly clean. Some of the things to usually check for include:\n",
    "* Missing values\n",
    "* Drastically different numerical ranges\n",
    "* Categorical variables\n",
    "\n",
    "Run the following script to prove to yourself that this dataset is fairly clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = iris_df.isnull().sum().rename('null_count')\n",
    "mins = iris_df.min().rename('min')\n",
    "maxs = iris_df.max().rename('max')\n",
    "pd.concat([nulls, mins, maxs], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one column that could be slightly cleaned up is the class column. Since this column represents 1 of 3 species of iris, it should probably be changed to a categorical data type. (*Note:* the impact of this will only be the pandas dataframe; often sklearn is happy to work with categorical targets as ints.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Original type: {iris_df.species.dtypes}')\n",
    "iris_df.loc[:,'species'] = pd.Categorical(iris_df.species)\n",
    "print(f'Corrected type: {iris_df.species.dtypes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Subsetting\n",
    "\n",
    "Once the data is processed, it's time to separate it into training and testing datasets (optionally other datasets). The point of this split is primarily to hold out data for validation. Because many models have the capability to overtrain (i.e. fit anomalies within the sample used for training), its possible they will assume that anomaly is consistent with the general population. Therefore, a testing dataset is used to verify overtraining has not occured and your model is still generalizable. Occasionally, intermediary holdouts are also useful to help direct hyper parameter tuning or model selection.\n",
    "\n",
    "Scikit-learn provides several helpful methods for splitting the original dataset into training and testing sets. The cell below shows you how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, shuffle=False)\n",
    "\n",
    "def show_split(x_train, x_test, y_train, y_test, n_samples):\n",
    "    print(f'train shapes: {x_train.shape} and {y_train.shape}; {len(y_train)/n_samples:.2f} % of total')\n",
    "    print(f' test shapes: {x_test.shape} and {y_test.shape}; {len(y_test)/n_samples:.2f} % of total')\n",
    "    print(f'first 20 training targets: {y_train[:20]}')\n",
    "    print(f'first 20  testing targets: {y_test[:20]}')\n",
    "show_split(x_train, x_test, y_train, y_test, len(iris.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrying out the split is fairly easy; however, be careful of the parameters you choose. Because I incorrectly chose to not shuffle before splitting, the test set consists of only one species which happens to also be only minimally present in the training set. This is one of the reasons why it's usually better to take the default and shuffle before splitting.\n",
    "\n",
    "**Exercise:** \n",
    "Fix my mistake by resplitting the dataset. This time, ensure it shuffles beforehand, set the random seed to 42, and make the test set 30% of the original dataset. The documentation (or `help(train_test_split)`) may prove helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = (None, None, None, None) # insert your call here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "This is one of the steps in machine learning that is often an artform. Most practitioners are able to limit the number of models but cannot say up front what the best model will be. Even exports are usually incorrect at the beginning of the process. However, for models and datasets where there's no issues retraining you don't need to be correct right out of the gate. Instead, pick a subset that is as small as you can reasonably limit the choices, and experiment! For performing the initial limitation, you'll want to know:\n",
    "\n",
    "1. What kind of problem am I trying to solve? (Regression, Classification, etc.)\n",
    "2. What kind of attributes are involved? (Discrete, Continuous, Image, Text, etc.)\n",
    "3. How many datapoints are there? (<100, <10K, <1M, etc.)\n",
    "\n",
    "These criteria will usually limit a lot of your choices. For those not familiar with the models, there are many flow charts available for guiding this process. Scikit learn has published a [flow chart](https://scikit-learn.org/stable/tutorial/machine_learning_map/) that specifically links to estimators implemented in the library.\n",
    "\n",
    "![Model Selection FlowChart for Scikit-Learn](flowchart.jpg)\n",
    "\n",
    "#### Exercise:\n",
    "Use the flowchart to decide on which models might be appropriate for the iris dataset (Hint: There should be 4 options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Now that we've selected a model, it's time to start training. This is the meat and potatoes of scikit learn. Every model in scikit-learn implements the [estimator](https://scikit-learn.org/stable/glossary.html#term-estimator) interface. An estimator exposes the `fit` method which expects the training dataset to be passed in, and methods for getting and setting parameters of the model. Estimator's used for prediction also expose a `predict` method, which can be passed the train, test (or future) datasets to gauge performance.\n",
    "\n",
    "As an example, let's use the Linear SVC Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(C=1.0, max_iter=10000)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the prediction on the training set in hand, we can assess how well we did both visuallly and numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "def show_metrics(y_train, y_pred):\n",
    "    print(f'Accuracy: {accuracy_score(y_train, y_pred)}')\n",
    "    fig, ax = plt.subplots()\n",
    "    confusion = confusion_matrix(y_train, y_pred)\n",
    "    ax.imshow(confusion, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    for i in range(confusion.shape[0]):\n",
    "        for j in range(confusion.shape[1]):\n",
    "            ax.text(j, i, confusion[i, j], ha='center', va='center', color='orange')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "show_metrics(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:**\n",
    "First choice was fairly accurate. But depending on the domain, 97% accuracy might not be good enough. Can we do better? Choose one of the three other models discussed in the Model Selection step and try fitting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your model constructor,\n",
    "# construct your model,\n",
    "# fit it, \n",
    "# and assign your new predictions to y_pred2\n",
    "show_metrics(y_train, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis\n",
    "\n",
    "Now that the model is tuned and we think it's trained sufficiently, it's time to determine how well we did. The process was started in the previous section, but there are still several outstanding questions to analyze:\n",
    "\n",
    "* Did we choose the best model?\n",
    "* For each model, did we choose the best set of hyper parameters?\n",
    "* How did the selected model perform on the test set?\n",
    "\n",
    "For all of these questions, a metric is needed to assess performance. Metrics in scikit-learn are generally specified in one of three ways:\n",
    "1. Many estimators expose a `score` method, which can be used to specify a default metric.\n",
    "2. Tools using cross-validation have a `scoring` parameter used for internal model evaluation.\n",
    "3. The metrics module contains functions that can be used for additional evaluation. Each of them expect at least two args, the true and predicted target values.\n",
    "\n",
    "#### Exercise \n",
    "We've used accuracy and shown the confusion matrix above, but this is not the only set of metrics that might make sense. Look through the metrics module [documentation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) and see if you can find another metric worth evaluating for this problem. Compute it for all of the models we've trained so far\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute your metrics here, for both y_pred and y_pred2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have metrics chosen, let's return to the unanswered questions. The first two questions both need our metric to be measured on more than the training set. Otherwise, we will not know if the model and hyper-parameters generalize well. So let's measure the metrics on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_metrics(y_test, model.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the test set has performed even better! We should note that with only 38 points in the test set, 1 point corresponds to a 2.6% shift. So really our accuracy should have *at least* a $\\pm1.3%$ uncertainty appended to it.\n",
    "\n",
    "Now, we could repeat this procedure for several sets of hyper parameters and several models and choose the best. By doing so we will lose the ability to use the test set performance as a measure of how well the model generalizes. Instead, these results can only be used to measure how well the trained parameters generalize within the model. This problem is one reason why intermediary test sets are separated out. Due to time constraints we will not investigate this problem. If you're interested in learning more, I'd recommend looking into cross-validation techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Now that we've gone through one complete project, it's time to do your own. Choose one of the two toy datasets we explored earlier.\n",
    "\n",
    "* [Wine Quality](https://scikit-learn.org/stable/datasets/index.html#wine-dataset)\n",
    "* [Handwritten Digits](https://scikit-learn.org/stable/datasets/index.html#digits-dataset)\n",
    "\n",
    "Create a separate notebook to work in and step through each of the steps outlined above. Once time is up, each group will give a short presentation showing what discoveries they made. The Jupyter notebook will be your presentation material, so document well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
