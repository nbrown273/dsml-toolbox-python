{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "The Pandas library is essentially a one-stop shop for common workflows in data science. It provides the basic toolset needed for data cleaning, feature engineering, statistical analysis, and visualizations in one place. Sometimes the basic toolset is enough, but in cases where it is not, the pandas library is built on top of numpy which allows easy integration with more specialized libraries like scikit-learn. Essentially, as long as it makes sense to represent your data in tables you should consider using pandas at least as a start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%logstart\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data\n",
    "Like with numpy arrays, once of the key skills is understanding how to create a pandas data structure in the first place. In pandas, unlike numpy, there are two fundamental types of data structures. The main data structure of Pandas is the DataFrame, which is a 2-D labeled, table like structure. It is composed of several Data Series, the other data structure in Pandas, which are 1-D homogeneous-typed arrays. Understanding how to work with these two structures is the core of working with Pandas.\n",
    "\n",
    "One way to get a dataframe is by importing from another data source. Pandas supports the ability to work with many common file formats (csv, excel, json, hdf5, ...) and databases. The cell below pulls a csv from the specified url and imports it in as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv'\n",
    "local = './iris.csv' # in case SSL fails\n",
    "iris = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as the file is formatted with the default dialect of csv (comma separated, double quoting, etc.) it's really that simple. However, the world is often not clean or standardized. In such cases, something must be configured to load the data succesfully.\n",
    "\n",
    "**EXERCISE:** Let's experiment creating a dataframe from JSON files. The simulated datasets are available in the module folder as `simulated_form<id>.json` files. Each file is the same dataset, but in a different json format. Use the `read_json` Pandas method to load three or more of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = None # fill in these lines\n",
    "df2 = None\n",
    "df3 = None\n",
    "\n",
    "for df in [df1, df2, df3]:\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to acquire DataFrames or Series is to literally construct them from existing collections. Base collections could be native Python lists, numpy arrays, or dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1, 3, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.random.randn(10,4), columns = ['A', 'B', 'C', 'D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"PI\": 3.14,\n",
    "    \"Radius\": np.arange(5),\n",
    "    \"Size\": pd.Categorical([\"S\", \"S\", \"S\", \"L\", \"L\"])\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data\n",
    "\n",
    "After you've created a data frame, the next natural step is exploring it's contents. There are several helpful methods that can be used for viewing different attributes. Below are just a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape = {iris.shape}')\n",
    "print(f'Index = {iris.index}')\n",
    "print(f'Columns = {iris.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.head())\n",
    "print(iris.tail())\n",
    "print(iris.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Import the (red) wine quality datasets and answer the following questions. \n",
    "1. How many columns and rows are there? \n",
    "2. How many columns deal with measures of acidity? \n",
    "3. What's the most alcholic wine available?\n",
    "\n",
    "The location where you can find the dataset has been provided for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "local = './winequality-red.csv'\n",
    "wine = None # insert your code here\n",
    "print(wine.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data\n",
    "\n",
    "Once the dataframe is created and a basic understanding of its contents established, the next most common steps are to examine subsets or perform transformations. In both cases, the ability to select specific elements of the DataFrame is key.\n",
    "\n",
    "There are four primary methods for selecting elements: `.loc()`, `.iloc()`, `[]`, and, in some cases, direct attribute access with `.`. The most efficient and explicit methods are`.loc()` and `.iloc()` for label and index based selections, respectively. Using `[]` and `.` is great for interactive work but is generally not recommended for production quality code. Here's a few examples with attribute access and indexing/slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.species.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['species'].head() # this will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[['petal_length', 'petal_width']].head() # so will this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['petal_length':] # this will not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[0] # this will fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[:5] # this will not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['species'][:5] # combining works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll flesh out the use of the selection by label and index methods, `loc` and `iloc` respectively. Both methods take the same set of five possible input types:\n",
    "1. a single label/index\n",
    "2. a list of labels/indicies\n",
    "3. a slice of labels/indicies\n",
    "4. a boolean array\n",
    "5. a callable object\n",
    "\n",
    "Any of these types can be used to specify the row(s) or column(s), or even both if an argument is provided for each. If only one is specified, it is assumed to identify the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.loc[:5, 'species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.loc[:5, ['petal_length', 'petal_width']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.loc[:5, 'petal_length':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.loc[iris.species == 'versicolor', :].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(iris.loc[lambda df : df.species == 'versicolor', :]\n",
    "    .loc[lambda df : df.petal_length < 4.5, :]\n",
    "    .head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.iloc[-5:,-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Let's investigate the wine dataset a little more. How many wines have the highest quality rating? Do this two ways:\n",
    "1. Call the value_counts method with no arguments on the pandas Series representing the quality column.\n",
    "2. Use selection by label to create a dataframe containing only the highest quality wines, then show the number of rows.\n",
    "\n",
    "Finally, add a new calculated column to your wine dataframe called 'best_in_show'. It should be true if the quality is the highest rating possible, and false otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operating on Data\n",
    "\n",
    "In order to process your data you'll need to know how to perform common statistical operations, apply custom functions, and create new dataframes (or modify the existing one). This includes handling missing data, computing statistics, reshaping (pivot, melt, transpose, etc.). Most of the operations are straightforward to use if you're familiar with the routine you're trying to apply. We'll explore a few examples here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_quality = pd.DataFrame([[1, np.nan], [2, 3.14]], columns = ['A', 'B'])\n",
    "poor_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_quality.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_quality.fillna(value = poor_quality.B.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_quality.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.iloc[:,:-1].apply(np.sum, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.iloc[:,:-1].apply(np.sum, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Use the `groupby` function on the iris dataset to compute stats using the `describe` function for each subset of species of iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data\n",
    "\n",
    "Underneath the hood, pandas is setup to take advantage of matplotlib's basic functionality. It then exposes that functionality through convenient methods of the DataFrame class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = iris.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = iris.plot(kind = 'scatter', x = 'petal_length', y='petal_width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "species = iris.groupby('species')\n",
    "for name, df in species:\n",
    "    ax.scatter(df.petal_length, df.petal_width, label=name)\n",
    "legend = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "We've been using the iris dataset a lot to acquiant ourselves with Pandas, but we only started with the wine data set. Your task now is to utilize all the methods you know to inspect the wine data set. Your goal is to gather ideas for what features or feature combinations might be useful for training a machine learning model to predict the quality of wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
